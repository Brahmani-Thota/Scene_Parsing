# Scene_Parsing
Objects are the core-building blocks of an image and generally, an image defines the relationship between objects. Visual Relation Detection involves detecting and localising pairs of objects in an image and also classifying the predicate or interaction between each pair. We define a model which predicts the relationship between objects, sometimes, these numbers might be huge. So, we concentrate on basic relationships ie., predicates which occur frequently. 
We propose a Visual Appearance Module that learns the appearance of objects and predicates and fuses them together to jointly predict relationships.  Additionally, we localize the objects in the predicted relationships as bounding boxes in the image. Another key observation is that relationships are semantically related to each other. For example, a “person riding a horse” and a “person riding an elephant” are semantically similar because both elephant and horse are animals. Word vector embeddings naturally lend themselves to linking such relationships because they capture semantic similarity in language. Therefore, we also have a Language Module that uses pre-trained word vectors to cast relationships into a vector space where similar relationships are optimized to be close to each other. Using this embedding space, we can finetune the prediction scores of our relationships and even enable zero shot relationship detection.
In this project, we have visual relationships by learning visual appearance models for its objects and predicates and using the relationship embedding space learnt from a language.
